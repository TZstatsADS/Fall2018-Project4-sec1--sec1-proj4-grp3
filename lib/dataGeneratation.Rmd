---
title: "Data and Feature Generation"
author: "Bailey Pierson"
date: "23/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load necceary packages
This script is written following the programming heuristics for "tidy" code in R, and requires the packages below:

```{r packages}
library('dplyr')
library('tidytext')
library('stringi')
library('readtext')
library('qdapDictionaries')
```

## Load in and segment data

We can now load in the ground truth and tesseract data files, then perform a series of operations to generate a set of correct terms through comparison to a dictionary commonly used for spelling correction (found in the `qdapDictionaries` package). The set of incorrect terms used for training if generated by selecting all terms that occur in the tesseract documents but NOT in the ground_truth documents.

```{r data_subsetting}
#read in data and qdap dictionary
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
data_tesseract <- readtext('../data/tesseract/*.txt', encoding='utf-8')
data("GradyAugmented")

#tokenize tesseract data
tokens_tesseract <- data_tesseract %>%
  select(-doc_id) %>%
  unnest_tokens(token, text) %>%
  distinct()

#tokenize ground_truth data
tokens_truth <- data_truth %>%
  select(-doc_id) %>%
  unnest_tokens(token, text) %>%
  distinct()

#create data subsets for training 
bad_terms <- subset(tokens_tesseract, !(tokens_tesseract$token %in% tokens_truth))
good_terms <- subset(tokens_truth, (nchar(tokens_truth$token) > 1 & (tokens_truth$token %in% GradyAugmented)))
```

## Compute character-level bigrams for feature generation 
One of the more complex features used in our model computes the how frequently the character bi-grams observed in a token occur in text that we know is correct. In order to compute this feature, the set of ground_truth documents are tokenized into character bi-grams and the bigram frequency is calculated relative to the entire corpus. This frequency is then used to compute the overall bigram frequency feature for all tokens in the tesseract and ground truth data sets. See our reference paper (paper D3) for further details, but note that our implementation differs slightly because our scaling factor is the total number of unique bi-grams in the truth corpus, rather than the value 10,000. I decided to alter our implementation because no reason was given for the use of 10,000, and our data set is considerably smaller than that used in the reference paper.

```{r char_bigrams}
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
  unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
  group_by(token) %>%
  summarise(frequency = (n())/1515) #1515 is number of unique character bigrams in ground_truth corpus

#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- data_truth %>%
  unnest_tokens(token, text) %>%
  mutate(token_id = token) %>%
  unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
  inner_join(char_bigrams, by = c('char' = 'token')) %>%
  group_by(token_id) %>%
  mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
  select(doc_id, token_id, bigramFreq) %>%
  distinct()
```

## Compute features for all tokens
See reference paper for detailed description of all features computed in the code below. Note that the Levenshtein distance feature was not computed, as was suggested in the project prompt.

```{r featureSet}
#combine 'good' and 'bad' tokens with class label added
good_terms <- good_terms %>% mutate(class = 0)
tokens <- bad_terms %>%
  mutate(class = 1) %>%
  bind_rows(good_terms)

#compute all features and save in new df
feature_set <- tokens %>%
  rowwise() %>% 
  mutate(len = nchar(token), 
         nvowels = str_count(tolower(token), "[aeoiu]"), 
         ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
         vowelRatio = nvowels/len, 
         consRatio = ncons/len, 
         vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0), 
         specialChar = str_count(token, "[^\\w\\d\\s:]"), 
         specialRatio = specialChar/len, 
         ndigits = str_count(token, "\\d"), 
         digitsRatio = ndigits/len,
         upperCase = str_count(token, "[A-Z]"), 
         lowerCase = str_count(token, "[a-z]"), 
         upperRatio = upperCase/len, 
         lowerRatio = lowerCase/len,
         seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
         majoritySpecial = ifelse(specialChar > floor(len/2), 1,0), 
         seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
         innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
         interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0), 
         countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
  left_join(bigramScore, by = c('token' = 'token_id')) %>%
  select(-interFreq, -doc_id.y) %>%
  distinct()
```

## Write generated data to correct directory
All data sets generated by this script are written to the /data directory

```{r write_data}
#write datasets to /data directory
write.csv(feature_set, '../data/featureSet.csv', row.names=F)
write.csv(bad_terms, '../data/badTerms.csv', row.names=F)
write.csv(good_terms, '../data/goodTerms.csv', row.names=F)
```