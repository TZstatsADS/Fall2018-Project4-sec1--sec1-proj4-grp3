calculate_psnr <- function(prediction, true_label){
if(length(prediction) != length(true_label)){
stop("input lists must be same length")
}
l <- vector("list", length(prediction))
for (i in 1:length(prediction)){
l[[i]] <- psnr(prediction[[i]], true_label[[i]])
}
return(unlist(l))
}
a <- readJPEG('Desktop/a.jpeg')
a1 <- readJPEG('Desktop/a1.jpeg')
b <- readJPEG('Desktop/b.jpeg')
b1 <- readJPEG('Desktop/b1.jpeg')
true <- list(a, b)
guess <- list(a1, b1)
ans <- psnr(true[[2]], guess[[2]])
is.infinite(ans)
psnr(true[[2]], guess[[2]])
ans <- psnr(true[[2]], guess[[2]])
mean(calculate_psnr(guess, true))
ans <- psnr(true[[2]], guess[[2]])
calculate_psnr(guess, true)
a <- readJPEG('Desktop/a.jpeg')
a1 <- readJPEG('Desktop/a1.jpeg')
b <- readJPEG('Desktop/b.jpeg')
b1 <- readJPEG('Desktop/b1.jpeg')
calculate_psnr(guess, true)
ans <- psnr(true[[1]], guess[[1]])
psnr <- function(x_hat,x) {
result = 10 * log10(255/mse(x_hat,x))
}
ans <- psnr(true[[1]], guess[[1]])
calculate_psnr(guess, true)
psnr <- function(x_hat,x) {
10 * log10(255/mse(x_hat,x))
}
ans <- psnr(true[[1]], guess[[1]])
calculate_psnr(guess, true)
calculate_psnr <- function(prediction, true_label){
if(length(prediction) != length(true_label)){
stop("input lists must be same length")
}
l <- vector("list", length(prediction))
for (i in 1:length(prediction)){
l[[i]] <- psnr(prediction[[i]], true_label[[i]])
}
return(l)
}
calculate_psnr(guess, true)
l_ans <- calculate_psnr(guess, true)
l_ans[1]
l_ans[2]
dat <- read.csv(file.choose())
View(dat)
library('tidy')
library('tidyverse')
library('dplyr')
ans <- dat %>%
filter(time = 0) %>%
summarise(n = mean(partner.depression))
ans <- dat %>%
filter(time == 0) %>%
summarise(n = mean(partner.depression))
View(ans)
ans <- dat %>%
filter(time == 0, partner.depression != NA) %>%
summarise(n = mean(partner.depression))
ans <- dat %>%
filter(time == 0, partner.depression != NA)
ans <- dat %>%
filter(time == 0.00000000, partner.depression != NA)
ans <- dat %>%
filter(time == 0)
ans <- dat %>%
filter(time == 0) %>%
mean(partner.depression, na.rm = T)
ans <- dat %>%
filter(time == 0) %>%
mean(partner.depression)
ans <- dat %>%
filter(time == 0)
ans$partner.depression
mean(ans$partner.depression)
mean(ans$partner.depression, na.rm = T)
dat %>%
filter(time == 0) %>%
summarise(mean = mean(partner.depression, na.rm = T))
data <- readtext('group1_00000005.txt', encoding='utf-8')
library('tidyverse')
library('dplyr')
library('tidytext')
library('stringi')
data <- readtext('group1_00000005.txt', encoding='utf-8')
??readtext
library('readtext')
data <- readtext('group1_00000005.txt', encoding='utf-8')
data <- readtext('group1_00000005.txt', encoding='utf-8')
getwd()
setwd("/Users/baileypierson/Desktop")
data <- readtext('group1_00000005.txt', encoding='utf-8')
tokens <- data %>%
unnest_tokens(token, text)
install.packages('rJava')
library('rJava')
CharacterNGramTokenizer <- structure(function (x, control = NULL)
{
tokenizer <- .jnew("weka/core/tokenizers/CharacterNGramTokenizer")
x <- Filter(nzchar, as.character(x))
if (!length(x))
return(character())
.jcall("RWekaInterfaces", "[S", "tokenize", .jcast(tokenizer,
"weka/core/tokenizers/Tokenizer"), .jarray(as.character(control)),
.jarray(as.character(x)))
}, class = c("R_Weka_tokenizer_interface", "R_Weka_interface"
), meta = structure(list(name = "weka/core/tokenizers/NGramTokenizer",
kind = "R_Weka_tokenizer_interface", class = "character",
init = NULL), .Names = c("name", "kind", "class", "init")))
BigramTokenizer <- function(x){
CharacterNGramTokenizer(x, Weka_control(min = 2, max = 2))}
CharacterNGramTokenizer <- structure(function (x, control = NULL)
{
tokenizer <- .jnew("weka/core/tokenizers/CharacterNGramTokenizer")
x <- Filter(nzchar, as.character(x))
if (!length(x))
return(character())
.jcall("RWekaInterfaces", "[S", "tokenize", .jcast(tokenizer,
"weka/core/tokenizers/Tokenizer"), .jarray(as.character(control)),
.jarray(as.character(x)))
}, class = c("R_Weka_tokenizer_interface", "R_Weka_interface"
), meta = structure(list(name = "weka/core/tokenizers/NGramTokenizer",
kind = "R_Weka_tokenizer_interface", class = "character",
init = NULL), .Names = c("name", "kind", "class", "init")))
library('rJava')
View(data)
View(tokens)
BigramTokenizer(tokens[1])
library('RWeka')
library('rJava')
library(quanteda)
install.packages('quanteda')
library('quanteda')
txts <- c("In this paper.", "In this lines this.")
tokens <- tokenize(gsub("\\s", "_", txts), "character", ngrams=4L, conc="")
tk <- tokenize(gsub("\\s", "_", txts), "character")
txts <- c('hiello')
tk <- tokenize(gsub("\\s", "_", txts), "character")
tk <- tokenize(gsub("\\s", "_", 'hello'), "character")
library('tm')
tk <- tokenize(gsub("\\s", "_", 'hello'), "character")
?tokenize
tk <- txts %>%
unnest_tokens(shingle, txt, token = "character_shingles", n = 2)
t <-data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2)
View(t)
str(t)
t <-data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = n())
View(t)
?readtext
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3')
data <- readtext('tesseract/data/*.txt', encoding='utf-8')
data <- readtext('*.txt', encoding='utf-8')
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/tesseract/data')
getwd()
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/data/tesseract')
data <- readtext('*.txt', encoding='utf-8')
View(data)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = n())
View(t)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = (n())/nrow())
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = (n())/1433)
View(t)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = nrow())
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/data/ground_truth')
#set wd to data/tesseract before running line below
data <- readtext('*.txt', encoding='utf-8')
View(data)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n()))
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515)
View(t)
char_bigrams <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515) #1515 is number of unique tokens in ground_truth corpus
t <- tokens %>%
unnest_tokens(char, text, token = "character_shingles", n = 2)
t <- tokens %>%
unnest_tokens(char, token, token = "character_shingles", n = 2)
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2)
View(t)
?inner_join
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('token_id' = 'char'))
View(char_bigrams)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token'))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
summarise(bigramFreq = sum(frequency))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token'))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
mutate(c = count(token_id))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(doc_id) %>%
mutate(c = n())
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n())
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
summarise(bigramFreq = (sum(frequency)/c))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
group_by(c) %>%
summarise(bigramFreq = (sum(frequency)/c))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
group_by(c)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n(), bigramFreq = sum(frequency))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), bigramFreq = sum(frequency), final = (bigramFreq/n))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq)
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
unique()
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
unique(token_id)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(t)
View(tokens)
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(test, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq)
View(full_dat)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y)
View(full_dat)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y) %>%
distinct()
View(full_dat)
getwd()
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/lib')
#read in data
data_truth <- readtext('../data/tesseract/*.txt', encoding='utf-8')
View(data_truth)
#read in data
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
data_tesseract <- readtext('../data/tesseract/*.txt', encoding='utf-8')
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515) #1515 is number of unique tokens in ground_truth corpus
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
#compute all features and save in new df
feature_set <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y) %>%
distinct()
View(feature_set)
write.csv(feature_set, "feature_dataset.csv")
getwd()
?write.csv
#create rough list of incorrect terms
tokens_truth <- data_truth %>%
select(-doc_id) %>%
unnest_tokens(token, text) %>%
distinct()
View(tokens_truth)
View(tokens)
bad_terms <- subset(tokens$token, !(tokens$token %in% tokens_truth)
bad_terms <- subset(tokens$token, !(tokens$token %in% tokens_truth))
head(bad_terms)
write.csv(bad_terms, 'bad_terms')
write.csv(bad_terms, 'bad_terms.csv')
write.csv(bad_terms, 'bad_terms.csv', row.names = F)
write.csv(feature_set, "feature_dataset.csv", row.names = F)
