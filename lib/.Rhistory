"weka/core/tokenizers/Tokenizer"), .jarray(as.character(control)),
.jarray(as.character(x)))
}, class = c("R_Weka_tokenizer_interface", "R_Weka_interface"
), meta = structure(list(name = "weka/core/tokenizers/NGramTokenizer",
kind = "R_Weka_tokenizer_interface", class = "character",
init = NULL), .Names = c("name", "kind", "class", "init")))
library('rJava')
View(data)
View(tokens)
BigramTokenizer(tokens[1])
library('RWeka')
library('rJava')
library(quanteda)
install.packages('quanteda')
library('quanteda')
txts <- c("In this paper.", "In this lines this.")
tokens <- tokenize(gsub("\\s", "_", txts), "character", ngrams=4L, conc="")
tk <- tokenize(gsub("\\s", "_", txts), "character")
txts <- c('hiello')
tk <- tokenize(gsub("\\s", "_", txts), "character")
tk <- tokenize(gsub("\\s", "_", 'hello'), "character")
library('tm')
tk <- tokenize(gsub("\\s", "_", 'hello'), "character")
?tokenize
tk <- txts %>%
unnest_tokens(shingle, txt, token = "character_shingles", n = 2)
t <-data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2)
View(t)
str(t)
t <-data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = n())
View(t)
?readtext
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3')
data <- readtext('tesseract/data/*.txt', encoding='utf-8')
data <- readtext('*.txt', encoding='utf-8')
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/tesseract/data')
getwd()
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/data/tesseract')
data <- readtext('*.txt', encoding='utf-8')
View(data)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = n())
View(t)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = (n())/nrow())
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(count = (n())/1433)
View(t)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = nrow())
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/data/ground_truth')
#set wd to data/tesseract before running line below
data <- readtext('*.txt', encoding='utf-8')
View(data)
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n()))
t <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515)
View(t)
char_bigrams <- data %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515) #1515 is number of unique tokens in ground_truth corpus
t <- tokens %>%
unnest_tokens(char, text, token = "character_shingles", n = 2)
t <- tokens %>%
unnest_tokens(char, token, token = "character_shingles", n = 2)
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2)
View(t)
?inner_join
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('token_id' = 'char'))
View(char_bigrams)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token'))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
summarise(bigramFreq = sum(frequency))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token'))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
mutate(c = count(token_id))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(doc_id) %>%
mutate(c = n())
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n())
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
summarise(bigramFreq = (sum(frequency)/c))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
group_by(c) %>%
summarise(bigramFreq = (sum(frequency)/c))
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n()) %>%
group_by(c)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(c = n(), bigramFreq = sum(frequency))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), bigramFreq = sum(frequency), final = (bigramFreq/n))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n))
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq)
View(t)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
unique()
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
unique(token_id)
t <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(t)
View(tokens)
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(test, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq)
View(full_dat)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y)
View(full_dat)
full_dat <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y) %>%
distinct()
View(full_dat)
getwd()
setwd('/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/lib')
#read in data
data_truth <- readtext('../data/tesseract/*.txt', encoding='utf-8')
View(data_truth)
#read in data
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
data_tesseract <- readtext('../data/tesseract/*.txt', encoding='utf-8')
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/1515) #1515 is number of unique tokens in ground_truth corpus
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
#compute all features and save in new df
feature_set <- tokens %>%
rowwise() %>%
mutate(len = nchar(token),
nvowels = str_count(tolower(token), "[aeoiu]"),
ncons = str_count(tolower(token), "[bcdfghjklmnpqrstvxzwy]"),
vowelRatio = nvowels/len,
consRatio = ncons/len,
vowelConsRatio = ifelse(ncons > 0, nvowels/ncons, 0),
specialChar = str_count(token, "[^\\w\\d\\s:]"),
specialRatio = specialChar/len,
ndigits = str_count(token, "\\d"),
digitsRatio = ndigits/len,
upperCase = str_count(token, "[A-Z]"),
lowerCase = str_count(token, "[a-z]"),
upperRatio = upperCase/len,
lowerRatio = lowerCase/len,
seqRatio = ifelse(str_count(token, "(.)\\1{2,}") > 0, ((nchar(str_extract(token, "(.)\\1{2,}")))/len),0),
majoritySpecial = ifelse(specialChar > floor(len/2), 1,0),
seqCons = ifelse(str_count(token, "(([b-df-hj-np-tv-z])(?!\2)){6}") > 0, 1, 0), #checks if more than 6 constants occur concurrently
innerSpecial = ifelse(str_count(gsub(".$", "", gsub("^.", "", token)), "[^\\w\\d\\s:]") > 1, 1, 0), #after removing first and last characters, checks if more than 1 special characters remain
interFreq = max(table(strsplit(token,"")))[1], mostFreq = ifelse(interFreq > 2, interFreq/len, 0),
countNonAlpha = (len -(nvowels + ncons))/(nvowels + ncons)) %>%
left_join(bigramScore, by = c('token' = 'token_id')) %>%
select(-interFreq, -doc_id.y) %>%
distinct()
View(feature_set)
write.csv(feature_set, "feature_dataset.csv")
getwd()
?write.csv
#create rough list of incorrect terms
tokens_truth <- data_truth %>%
select(-doc_id) %>%
unnest_tokens(token, text) %>%
distinct()
View(tokens_truth)
View(tokens)
bad_terms <- subset(tokens$token, !(tokens$token %in% tokens_truth)
bad_terms <- subset(tokens$token, !(tokens$token %in% tokens_truth))
head(bad_terms)
write.csv(bad_terms, 'bad_terms')
write.csv(bad_terms, 'bad_terms.csv')
write.csv(bad_terms, 'bad_terms.csv', row.names = F)
write.csv(feature_set, "feature_dataset.csv", row.names = F)
#read in data
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
#load neccesary packages
library('tidyverse')
library('stringi')
library('readtext')
#read in data
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token)
#load neccesary packages
library('tidytext')
library('dplyr')
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token)
#tokenize tesseract data
tokens <- data_truth %>%
unnest_tokens(token, text)
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/297034) #1515 is number of unique tokens in ground_truth corpus
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
data_tesseract <- readtext('../data/tesseract/*.txt', encoding='utf-8')
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/10000) #297034 is number of unique tokens in ground_truth corpus
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
#tokenize tesseract data
tokens <- data_truth %>%
unnest_tokens(token, text) %>%
distinct(token)
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
group_by(token) %>%
summarise(frequency = (n())/17542) #297034 is number of unique tokens in ground_truth corpus
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text) %>%
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text) %>%
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
#tokenize tesseract data
tokens <- data_tesseract %>%
unnest_tokens(token, text)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct(token_id)
View(bigramScore)
#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- tokens %>%
mutate(token_id = token) %>%
unnest_tokens(char, token, token = "character_shingles", n = 2) %>%
inner_join(char_bigrams, by = c('char' = 'token')) %>%
group_by(token_id) %>%
mutate(n = n(), termFreq = sum(frequency), bigramFreq = (termFreq/n)) %>%
select(doc_id, token_id, bigramFreq) %>%
distinct()
View(bigramScore)
View(char_bigrams)
