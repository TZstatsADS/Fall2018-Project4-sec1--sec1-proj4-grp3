---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Compute model features

## Load neccesary packages
This script is written following the programming heuristics for "tidy" code in R, and requires the packages below:

```{r packages, warning = FALSE}
library('dplyr')
library('tidytext')
library('stringi')
library('readtext')
library('stringr')
library('qdapDictionaries')
library('e1071')
library('MlBayesOpt')
```

## Load in and segment data

We can now load in the ground truth and tesseract data files, then perform a series of operations to generate a set of correct terms through comparison to a dictionary commonly used for spelling correction (found in the `qdapDictionaries` package). The set of incorrect terms used for training if generated by selecting all terms that occur in the tesseract documents but NOT in the ground_truth documents.

```{r data_subsetting}
setwd("/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/doc")

#read in data and qdap dictionary
data_truth <- readtext('../data/ground_truth/*.txt', encoding='utf-8')
data_tesseract <- readtext('../data/tesseract/*.txt', encoding='utf-8')
data("GradyAugmented")

#tokenize tesseract data
tokens_tesseract <- data_tesseract %>%
  select(-doc_id) %>%
  unnest_tokens(token, text, to_lower = F) %>%
  distinct()

#tokenize ground_truth data
tokens_truth <- data_truth %>%
  select(-doc_id) %>%
  unnest_tokens(token, text, to_lower = F) %>%
  distinct()

#create data subsets for training 
bad_terms <- subset(tokens_tesseract, !(tokens_tesseract$token %in% tokens_truth))
good_terms <- subset(tokens_truth, (nchar(tokens_truth$token) > 1 & (tokens_truth$token %in% GradyAugmented)))

#add class label
good_terms <- good_terms %>% mutate(class = 0)
tokens <- bad_terms %>%
  mutate(class = 1) %>%
  bind_rows(good_terms)

#read in matched_pairs of tokens from tesseract and ground_truth corpus
matched_pairs <- read.csv('../data/matched_pairs', stringsAsFactors = F)
names(matched_pairs) <- c('pair_id', 'token', 'ground_t')

#add class label
matched_pairs <- mutate(matched_pairs, class = ifelse(token == ground_t, 0, 1))
```

## Compute character-level bigrams for feature generation 
One of the more complex features used in our model computes the how frequently the character bi-grams observed in a token occur in text that we know is correct. In order to compute this feature, the set of ground_truth documents are tokenized into character bi-grams and the bigram frequency is calculated relative to the entire corpus. This frequency is then used to compute the overall bigram frequency feature for all tokens in the tesseract and ground truth data sets. See our reference paper (paper D3) for further details, but note that our implementation differs slightly because our scaling factor is the total number of unique bi-grams in the truth corpus, rather than the value 10,000. I decided to alter our implementation because no reason was given for the use of 10,000, and our data set is considerably smaller than that used in the reference paper.

```{r char_bigrams}
source('../lib/functions.R')
#create list of character bigrams frequencies from ground_truth data
char_bigrams <- data_truth %>%
  unnest_tokens(token, text, token = "character_shingles", n = 2) %>%
  group_by(token) %>%
  summarise(frequency = (n())/1515) #1515 is number of unique character bigrams in ground_truth corpus

#compute bigramFreq feature using ground truth character bigram frequency
bigramScore <- generateBigramscore(tokens)

#compute bigramFreq for matched_pairs data using ground truth character bigram frequency
bigramScoreMatched <- generateBigramscore(matched_pairs)
```

## Compute features for all tokens
See reference paper for detailed description of all features computed in the code below. Note that the Levenshtein distance feature was not computed, as was suggested in the project prompt.

### 
```{r featureSet}
#compute all features and save in new df
feature_set <- generateFeatures(bigramScore)
feature_set_matched <- generateFeatures(bigramScoreMatched)
```

## Write generated data to correct directory
The feature set generated by this script is written to the /data directory

```{r write_data}
#write datasets to /data directory
write.csv(feature_set, '../data/featureSet.csv', row.names=F)
write.csv(feature_set_matched, '../data/featureSetMatched.csv', row.names=F)
```

# Fit SVM

Still following the specifications of the D3 paper, we now use our generated feature set data to fit an SVM (using the LIBSVM implementation) with a radial basis function kernel.

## Load and processing data for modeling
We first load our set of tokens and features, then remove columns that are *not* features we want to include in our SVM. In addition, we also remove the majoritySpecial feature, as it is constant across all of our data. As a reminder, the majoritySpecial was a flag for is the majority of characters in a string were alphanumeric.

```{r svm_processing}
setwd("/Users/baileypierson/Code/Fall2018-Project4-sec1--sec1-proj4-grp3/doc")

#read in and split data into test and training sets
feature_set_matched <- read.csv( '../data/featureSetMatched.csv', stringsAsFactors = F)
feature_set_matched <- feature_set_matched %>%
  select(-token_id, -majoritySpecial) %>% #we also remove the MajoritySpecial feature because it is constant (ie. 0 for all tokens)
  mutate(class = as.factor(class))

test_matched <- sample_frac(feature_set_matched, .25) #randomly set aside 25% of data for testing
index_matched <- c(as.numeric(rownames(test_matched)))
train_matched <- feature_set_matched[-index_matched,] #make sure our testing data is not in our training set

#same process for unmatched data
feature_set <- read.csv( '../data/featureSet.csv', stringsAsFactors = F)
feature_set <- feature_set %>%
  select(-token_id, -majoritySpecial, -countNonAlpha) %>% #we also remove the MajoritySpecial feature because it is constant (ie. 0 for all tokens), and countNonAlpha as it contains infty values
  mutate(class = as.factor(class))

test <- sample_frac(feature_set, .25) #randomly set aside 25% of data for testing
index <- c(as.numeric(rownames(test)))
train <- feature_set[-index,]
```

## Tune SVM
We determine the best hyperparameter values for our model using Bayesian optimization based on gaussian processes. The common `sv.tune()` function used for hyperparameter tuning with the `e1071` package relies on a grid search in order to determine the hyperparameters which minimize the classification error (ie. the 'best' hyperparameters), but this kind of exhaustive search is very computationally expensive. Instead, Bayesian optimization constructs a posterior distribution of functions that describes the function you want to optimize for, then searches the point whose error may be smaller.

```{r svm_tuning}
#first let's just run the model with the default hyperparameter values and cross folding for accuracy measures
svm.model.matched <- svm(class ~., feature_set_matched, cross = 5)
summary(svm.model.matched) #Accuracy around 74%

#tune model with bayes optimization
svm.tune.matched <- svm_cv_opt(data = train_matched, label = class, degree_range = c(2L, 4L), 
                       n_folds = 3, kappa = 5, init_points = 4, n_iter = 5)
svm.tune.matched$Best_Par #these are the optimal hyperparameter values

#same process for unmatched data
svm.model<- svm(class ~., feature_set, cross = 5)
summary(svm.model)  #Accuracy around 79%

#tune unmatched model with bayes optimization
svm.tune <- svm_cv_opt(data = train, label = class, degree_range = c(2L, 4L), 
                       n_folds = 3, kappa = 5, init_points = 4, n_iter = 5)
svm.tune$Best_Par #these are the optimal hyperparameter values, but they dont seem to give much of an accuracy improvement
```

## Predict values and compute accuracy
Finally, we can use the training and test data segmented above to compute the overall accuracy of the SVM with the best hyperparameters
```{r svm_predict}
svm.model.tuned.match <- svm(class ~., train_matched, gamma = 0.0010, cost = 5.4611) 
svm.pred.tuned.match <- predict(svm.model.tuned.match, test_matched[,-1]) #remove column of class labels
table(pred = svm.pred.tuned.match, true = test_matched$class) #Accuracy around 77%, an improvement!

#we use the default parameters for the unmatched data as they perform roughly the same (and are close in value), to the tuned parameters
svm.model.tuned <- svm(class ~., train) 
svm.pred.tuned <- predict(svm.model.tuned, test[,-1]) #remove column of class labels
table(pred = svm.pred.tuned, true = test$class) #Accuracy around 80%
```

# Compute Precision/Recall Metrics
While we cannot actually compute the overall word and character-level metrics because the scope of my assinged work did not involve the final error correction model, I did write a script for computing character-level metrics for the rest of my team. I included it below:

```{r performance_metrics}
char_difference <- function(ground_t,tesseract){
  ground_t <- as.character(ground_t)
  tesseract <- as.character(tesseract)
  n = 0
  if(ground_t == tesseract){
    return(nchar(ground_t))
  }else{
    char_ground_t <- as.vector(unlist(strsplit(ground_t,"")))
    char_tesseract <- as.vector(unlist(strsplit(tesseract,"")))
    min_length <- min(length(char_ground_t), length(char_tesseract))
    max_length <- max(length(char_ground_t), length(char_tesseract))
    n <- length(intersect(char_ground_t, char_tesseract))
    output <- list(n,max_length)
    return(output[1])
  }  
}
```